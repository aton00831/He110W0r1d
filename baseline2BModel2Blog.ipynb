{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(367028, 772)\n",
      "(367028, 1)\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "\n",
    "train_malicious = np.genfromtxt(\"./JSF20170324/malicious.vlog\", delimiter=',')\n",
    "train_normal    = np.genfromtxt(\"./JSF20170324/normal.vlog\"   , delimiter=',')\n",
    "\n",
    "X = np.concatenate((train_malicious, train_normal))\n",
    "y = np.array([1]*len(train_malicious) + [0]*len(train_normal))\n",
    "del(train_malicious)\n",
    "del(train_normal)\n",
    "y = np.reshape(y, [len(y), 1])\n",
    "X = np.delete(X, np.s_[:1], axis=1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348676, 772)\n",
      "(18352, 772)\n",
      "(348676, 1)\n",
      "(18352, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=831\n",
    ")\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.log(X_train+1)\n",
    "X_valid = np.log(X_valid+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\">>> model = Model(\"Batch Norm\", 32, 10)\n",
    "    \"\"\"\n",
    "    def __init__(self, name, input_dim, output_dim, hidden_dims=[32, 32], use_batchnorm=True, activation_fn=tf.nn.relu, optimizer=tf.train.AdamOptimizer, lr=0.01):\n",
    "        \"\"\" Constructor\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            # Placeholders are defined\n",
    "            with tf.name_scope(name+'_input'):\n",
    "                self.X = tf.placeholder(tf.float32, [None, input_dim], name='X')\n",
    "                self.y = tf.placeholder(tf.float32, [None, output_dim], name='y')\n",
    "                self.mode = tf.placeholder(tf.bool, name='train_mode')            \n",
    "            \n",
    "            # Loop over hidden layers\n",
    "            net = self.X\n",
    "            for i, h_dim in enumerate(hidden_dims):\n",
    "                with tf.variable_scope('layer{}'.format(i)):\n",
    "                    net = tf.layers.dense(net, h_dim)\n",
    "                    \n",
    "                    if use_batchnorm:\n",
    "                        net = tf.layers.batch_normalization(net, training=self.mode)\n",
    "                        \n",
    "                    net = activation_fn(net)\n",
    "            \n",
    "            # Attach fully connected layers\n",
    "            net = tf.contrib.layers.flatten(net)\n",
    "            net = tf.layers.dense(net, output_dim)\n",
    "            \n",
    "            # self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=self.y)\n",
    "            with tf.name_scope(name+'_loss'):\n",
    "                self.loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=net, labels=self.y)\n",
    "                # http://stackoverflow.com/questions/34240703/difference-between-tensorflow-tf-nn-softmax-and-tf-nn-softmax-cross-entropy-with\n",
    "                self.loss = tf.reduce_mean(self.loss, name='loss')    \n",
    "\n",
    "            with tf.name_scope(name+'_train'):\n",
    "                # When using the batchnormalization layers,\n",
    "                # it is necessary to manually add the update operations\n",
    "                # because the moving averages are not included in the graph            \n",
    "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)\n",
    "                with tf.control_dependencies(update_ops):                     \n",
    "                    self.train_op = optimizer(lr).minimize(self.loss)\n",
    "            \n",
    "            with tf.name_scope(name+'_Accuracy'):\n",
    "                self.predicted = tf.cast(net > 0.5, dtype=tf.float32)\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predicted, self.y), dtype=tf.float32))\n",
    "            \n",
    "            with tf.name_scope(name+'_ROC'):\n",
    "                predi_bool = tf.cast(self.predicted, tf.bool)\n",
    "                label_bool = tf.cast(self.y, tf.bool)\n",
    "                # for TP\n",
    "                TP_and = tf.logical_and(predi_bool, label_bool)\n",
    "                # for TN\n",
    "                tmp_or = tf.logical_or(predi_bool, label_bool)\n",
    "                TN_not = tf.logical_not(tmp_or)\n",
    "                # for FP\n",
    "                label_not = tf.logical_not(label_bool)\n",
    "                FP_and = tf.logical_and(predi_bool, label_not)\n",
    "                # for FN\n",
    "                predi_not = tf.logical_not(predi_bool)\n",
    "                FN_and = tf.logical_and(predi_not, label_bool)\n",
    "                # Add node\n",
    "                self.TP = tf.reduce_sum(tf.cast(TP_and, tf.float32))\n",
    "                self.TN = tf.reduce_sum(tf.cast(TN_not, tf.float32))\n",
    "                self.FP = tf.reduce_sum(tf.cast(FP_and, tf.float32))\n",
    "                self.FN = tf.reduce_sum(tf.cast(FN_and, tf.float32))\n",
    "            \n",
    "                # Precision Recall [ref](https://argcv.com/articles/1036.c)\n",
    "                # wiki got better pic [wiki](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "                self.precision = self.TP / (self.TP + self.FP)\n",
    "                self.recall    = self.TP / (self.TP + self.FN)\n",
    "                \n",
    "            # create a summary for our cost and accuracy \n",
    "            tf.summary.scalar(\"cost\", self.loss)\n",
    "            tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "            tf.summary.scalar(\"precision\", self.precision)\n",
    "            tf.summary.scalar(\"recall\", self.recall)\n",
    "            \n",
    "            # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "            # self.merged = tf.summary.scalar(\"cost\", self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    \"\"\"Solver class\n",
    "    \n",
    "    This class will contain the model class and session\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    model : Model class\n",
    "    sess : TF session\n",
    "        \n",
    "    Methods\n",
    "    ----------\n",
    "    train(X, y)\n",
    "        Run the train_op and Returns the loss\n",
    "        \n",
    "    evalulate(X, y, batch_size=None)\n",
    "        Returns \"Loss\" and \"Accuracy\"\n",
    "        If batch_size is given, it's computed using batch_size\n",
    "        because most GPU memories cannot handle the entire training data at once\n",
    "            \n",
    "    Example\n",
    "    ----------\n",
    "    >>> sess = tf.InteractiveSession()\n",
    "    >>> model = Model(\"BatchNorm\", 32, 10)\n",
    "    >>> solver = Solver(sess, model)\n",
    "    \n",
    "    # Train\n",
    "    >>> solver.train(X, y)\n",
    "    \n",
    "    # Evaluate\n",
    "    >>> solver.evaluate(X, y)\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, model, writer):\n",
    "        self.model = model\n",
    "        self.sess = sess\n",
    "        self.writer = writer\n",
    "        \n",
    "        self.writer.add_graph(sess.graph)\n",
    "    def train(self, X, y):\n",
    "        feed = {\n",
    "            self.model.X: X,\n",
    "            self.model.y: y,\n",
    "            self.model.mode: True\n",
    "        }\n",
    "        train_op = self.model.train_op\n",
    "        loss = self.model.loss\n",
    "        \n",
    "        return self.sess.run([train_op, loss], feed_dict=feed)\n",
    "    \n",
    "    def evaluate(self, X, y, batch_size=None):\n",
    "        if batch_size:\n",
    "            N = X.shape[0]\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            total_TP = 0\n",
    "            total_TN = 0\n",
    "            total_FP = 0\n",
    "            total_FN = 0\n",
    "            for i in range(0, N, batch_size):\n",
    "                \n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                feed = {\n",
    "                    self.model.X: X_batch,\n",
    "                    self.model.y: y_batch,\n",
    "                    self.model.mode: False\n",
    "                }\n",
    "                \n",
    "                loss = self.model.loss\n",
    "                accuracy = self.model.accuracy\n",
    "                \n",
    "                TP = self.model.TP\n",
    "                TN = self.model.TN\n",
    "                FP = self.model.FP\n",
    "                FN = self.model.FN\n",
    "                \n",
    "                merged = self.model.merged\n",
    "                \n",
    "                summary, step_loss, step_acc, step_tp, step_tn, step_fp, step_fn = \\\n",
    "                    self.sess.run([merged, loss, accuracy, TP, TN, FP, FN], feed_dict=feed)\n",
    "                \n",
    "                total_loss += step_loss * X_batch.shape[0]\n",
    "                total_acc += step_acc * X_batch.shape[0]\n",
    "                total_TP += step_tp\n",
    "                total_TN += step_tn\n",
    "                total_FP += step_fp\n",
    "                total_FN += step_fn\n",
    "                \n",
    "                self.writer.add_summary(summary, i)\n",
    "            total_loss /= N\n",
    "            total_acc /= N\n",
    "            return (total_loss, total_acc, total_TP, total_TN, total_FP, total_FN)\n",
    "            \n",
    "        else:\n",
    "            feed = {\n",
    "                self.model.X: X,\n",
    "                self.model.y: y,\n",
    "                self.model.mode: False\n",
    "            }\n",
    "            \n",
    "            loss = self.model.loss            \n",
    "            accuracy = self.model.accuracy\n",
    "            TP = self.model.TP\n",
    "            TN = self.model.TN\n",
    "            FP = self.model.FP\n",
    "            FN = self.model.FN\n",
    "\n",
    "            return self.sess.run([loss, accuracy, TP, TN, FP, FN], feed_dict=feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_dim = 772\n",
    "output_dim = 1\n",
    "# N = 55000\n",
    "N = len(X_train)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "# We create two models: one with the batch norm and other without\n",
    "# def __init__(self, name, input_dim, output_dim, hidden_dims=[32, 32], use_batchnorm=True, activation_fn=tf.nn.relu, optimizer=tf.train.AdamOptimizer, lr=0.01):\n",
    "bn = Model('batchnorm', input_dim, output_dim, hidden_dims=[1024, 1024, 512, 512, 256, 256, 64], use_batchnorm=True)\n",
    "# nn = Model('no_norm', input_dim, output_dim, hidden_dims=[500, 250, 250, 100], use_batchnorm=False)\n",
    "# bn = Model('batchnorm', input_dim, output_dim, hidden_dims=[2,2], use_batchnorm=True)\n",
    "#nn = Model('no_norm', input_dim, output_dim, hidden_dims=[2,2], use_batchnorm=False)\n",
    "\n",
    "# tensorboard setting \n",
    "logs_path = \"./file_writer/\"\n",
    "# tensorboard --logdir=path/to/log-directory\n",
    "bn_file_writer = tf.summary.FileWriter(logs_path+\"bnlog/\")\n",
    "#nn_file_writer = tf.summary.FileWriter(logs_path+\"nn/\")\n",
    "\n",
    "# We create two solvers: to train both models at the same time for comparison\n",
    "# Usually we only need one solver class\n",
    "bn_solver = Solver(sess, bn, bn_file_writer)\n",
    "#nn_solver = Solver(sess, nn, nn_file_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epoch_n = 300\n",
    "##batch_size = 32\n",
    "batch_size = 500\n",
    "\n",
    "# Save Losses and Accuracies every epoch\n",
    "# We are going to plot them later\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "train_ROC = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "valid_ROC = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batch :  697\n",
      "Total traning data:  348676\n",
      "Batch size : 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batch : \", N//batch_size)\n",
    "print(\"Total traning data: \", N)\n",
    "print(\"Batch size :\", batch_size)\n",
    "\n",
    "def get_minibatches_index(n, batch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    Used to shjuffle the dataset at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // batch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                        minibatch_start + batch_size])\n",
    "        minibatch_start += batch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "minibatches = get_minibatches_index(len(X_train), batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348676, 772)\n",
      "(348676, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0-TRAIN] Batchnorm Loss(Acc): 0.04096(0.99)\n",
      "Batchnorm Loss(Acc): 0.04096(0.99), TP: 131557, TN: 213166, FP, 543, FN: 3410\n",
      "[Epoch 0-VALID] Batchnorm Loss(Acc): 0.04420(0.99)\n",
      "Batchnorm Loss(Acc): 0.04420(0.99), TP: 6987, TN: 11149, FP, 34, FN: 182\n",
      "\n",
      "[Epoch 1-TRAIN] Batchnorm Loss(Acc): 0.08117(0.98)\n",
      "Batchnorm Loss(Acc): 0.08117(0.98), TP: 128802, TN: 213310, FP, 399, FN: 6165\n",
      "[Epoch 1-VALID] Batchnorm Loss(Acc): 0.08113(0.98)\n",
      "Batchnorm Loss(Acc): 0.08113(0.98), TP: 6845, TN: 11158, FP, 25, FN: 324\n",
      "\n",
      "[Epoch 2-TRAIN] Batchnorm Loss(Acc): 0.03391(0.99)\n",
      "Batchnorm Loss(Acc): 0.03391(0.99), TP: 132272, TN: 213027, FP, 682, FN: 2695\n",
      "[Epoch 2-VALID] Batchnorm Loss(Acc): 0.03832(0.99)\n",
      "Batchnorm Loss(Acc): 0.03832(0.99), TP: 7021, TN: 11146, FP, 37, FN: 148\n",
      "\n",
      "[Epoch 3-TRAIN] Batchnorm Loss(Acc): 0.13859(0.96)\n",
      "Batchnorm Loss(Acc): 0.13859(0.96), TP: 120453, TN: 213518, FP, 191, FN: 14514\n",
      "[Epoch 3-VALID] Batchnorm Loss(Acc): 0.13995(0.96)\n",
      "Batchnorm Loss(Acc): 0.13995(0.96), TP: 6408, TN: 11173, FP, 10, FN: 761\n",
      "\n",
      "[Epoch 4-TRAIN] Batchnorm Loss(Acc): 0.02339(0.99)\n",
      "Batchnorm Loss(Acc): 0.02339(0.99), TP: 133094, TN: 213151, FP, 558, FN: 1873\n",
      "[Epoch 4-VALID] Batchnorm Loss(Acc): 0.02738(0.99)\n",
      "Batchnorm Loss(Acc): 0.02738(0.99), TP: 7072, TN: 11147, FP, 36, FN: 97\n",
      "\n",
      "[Epoch 5-TRAIN] Batchnorm Loss(Acc): 0.03116(0.99)\n",
      "Batchnorm Loss(Acc): 0.03116(0.99), TP: 131073, TN: 213398, FP, 311, FN: 3894\n",
      "[Epoch 5-VALID] Batchnorm Loss(Acc): 0.03325(0.99)\n",
      "Batchnorm Loss(Acc): 0.03325(0.99), TP: 6959, TN: 11163, FP, 20, FN: 210\n",
      "\n",
      "[Epoch 6-TRAIN] Batchnorm Loss(Acc): 0.02033(0.99)\n",
      "Batchnorm Loss(Acc): 0.02033(0.99), TP: 133087, TN: 213113, FP, 596, FN: 1880\n",
      "[Epoch 6-VALID] Batchnorm Loss(Acc): 0.02157(0.99)\n",
      "Batchnorm Loss(Acc): 0.02157(0.99), TP: 7059, TN: 11149, FP, 34, FN: 110\n",
      "\n",
      "[Epoch 7-TRAIN] Batchnorm Loss(Acc): 0.04615(0.99)\n",
      "Batchnorm Loss(Acc): 0.04615(0.99), TP: 130846, TN: 213093, FP, 616, FN: 4121\n",
      "[Epoch 7-VALID] Batchnorm Loss(Acc): 0.05161(0.99)\n",
      "Batchnorm Loss(Acc): 0.05161(0.99), TP: 6949, TN: 11154, FP, 29, FN: 220\n",
      "\n",
      "[Epoch 8-TRAIN] Batchnorm Loss(Acc): 0.02667(0.99)\n",
      "Batchnorm Loss(Acc): 0.02667(0.99), TP: 131224, TN: 213406, FP, 303, FN: 3743\n",
      "[Epoch 8-VALID] Batchnorm Loss(Acc): 0.02993(0.99)\n",
      "Batchnorm Loss(Acc): 0.02993(0.99), TP: 6972, TN: 11167, FP, 16, FN: 197\n",
      "\n",
      "[Epoch 9-TRAIN] Batchnorm Loss(Acc): 0.03512(0.99)\n",
      "Batchnorm Loss(Acc): 0.03512(0.99), TP: 130953, TN: 213418, FP, 291, FN: 4014\n",
      "[Epoch 9-VALID] Batchnorm Loss(Acc): 0.04119(0.99)\n",
      "Batchnorm Loss(Acc): 0.04119(0.99), TP: 6946, TN: 11161, FP, 22, FN: 223\n",
      "\n",
      "[Epoch 10-TRAIN] Batchnorm Loss(Acc): 0.03186(0.99)\n",
      "Batchnorm Loss(Acc): 0.03186(0.99), TP: 132268, TN: 213235, FP, 474, FN: 2699\n",
      "[Epoch 10-VALID] Batchnorm Loss(Acc): 0.03691(0.99)\n",
      "Batchnorm Loss(Acc): 0.03691(0.99), TP: 7020, TN: 11155, FP, 28, FN: 149\n",
      "\n",
      "[Epoch 11-TRAIN] Batchnorm Loss(Acc): 0.02258(0.99)\n",
      "Batchnorm Loss(Acc): 0.02258(0.99), TP: 132313, TN: 213201, FP, 508, FN: 2654\n",
      "[Epoch 11-VALID] Batchnorm Loss(Acc): 0.02791(0.99)\n",
      "Batchnorm Loss(Acc): 0.02791(0.99), TP: 7030, TN: 11149, FP, 34, FN: 139\n",
      "\n",
      "[Epoch 12-TRAIN] Batchnorm Loss(Acc): 0.03242(0.99)\n",
      "Batchnorm Loss(Acc): 0.03242(0.99), TP: 131596, TN: 213494, FP, 215, FN: 3371\n",
      "[Epoch 12-VALID] Batchnorm Loss(Acc): 0.03948(0.99)\n",
      "Batchnorm Loss(Acc): 0.03948(0.99), TP: 6973, TN: 11172, FP, 11, FN: 196\n",
      "\n",
      "[Epoch 13-TRAIN] Batchnorm Loss(Acc): 0.03528(0.99)\n",
      "Batchnorm Loss(Acc): 0.03528(0.99), TP: 131755, TN: 213212, FP, 497, FN: 3212\n",
      "[Epoch 13-VALID] Batchnorm Loss(Acc): 0.04187(0.99)\n",
      "Batchnorm Loss(Acc): 0.04187(0.99), TP: 7002, TN: 11159, FP, 24, FN: 167\n",
      "\n",
      "[Epoch 14-TRAIN] Batchnorm Loss(Acc): 0.01907(0.99)\n",
      "Batchnorm Loss(Acc): 0.01907(0.99), TP: 132910, TN: 213453, FP, 256, FN: 2057\n",
      "[Epoch 14-VALID] Batchnorm Loss(Acc): 0.02675(0.99)\n",
      "Batchnorm Loss(Acc): 0.02675(0.99), TP: 7051, TN: 11168, FP, 15, FN: 118\n",
      "\n",
      "[Epoch 15-TRAIN] Batchnorm Loss(Acc): 0.02044(0.99)\n",
      "Batchnorm Loss(Acc): 0.02044(0.99), TP: 133598, TN: 213199, FP, 510, FN: 1369\n",
      "[Epoch 15-VALID] Batchnorm Loss(Acc): 0.02816(0.99)\n",
      "Batchnorm Loss(Acc): 0.02816(0.99), TP: 7089, TN: 11143, FP, 40, FN: 80\n",
      "\n",
      "[Epoch 16-TRAIN] Batchnorm Loss(Acc): 0.01691(0.99)\n",
      "Batchnorm Loss(Acc): 0.01691(0.99), TP: 133426, TN: 213421, FP, 288, FN: 1541\n",
      "[Epoch 16-VALID] Batchnorm Loss(Acc): 0.02373(0.99)\n",
      "Batchnorm Loss(Acc): 0.02373(0.99), TP: 7080, TN: 11165, FP, 18, FN: 89\n",
      "\n",
      "[Epoch 17-TRAIN] Batchnorm Loss(Acc): 0.02155(0.99)\n",
      "Batchnorm Loss(Acc): 0.02155(0.99), TP: 133142, TN: 213229, FP, 480, FN: 1825\n",
      "[Epoch 17-VALID] Batchnorm Loss(Acc): 0.02907(0.99)\n",
      "Batchnorm Loss(Acc): 0.02907(0.99), TP: 7066, TN: 11155, FP, 28, FN: 103\n",
      "\n",
      "[Epoch 18-TRAIN] Batchnorm Loss(Acc): 0.01221(1.00)\n",
      "Batchnorm Loss(Acc): 0.01221(1.00), TP: 134227, TN: 213251, FP, 458, FN: 740\n",
      "[Epoch 18-VALID] Batchnorm Loss(Acc): 0.01952(1.00)\n",
      "Batchnorm Loss(Acc): 0.01952(1.00), TP: 7123, TN: 11151, FP, 32, FN: 46\n",
      "\n",
      "[Epoch 19-TRAIN] Batchnorm Loss(Acc): 0.01289(1.00)\n",
      "Batchnorm Loss(Acc): 0.01289(1.00), TP: 134364, TN: 212974, FP, 735, FN: 603\n",
      "[Epoch 19-VALID] Batchnorm Loss(Acc): 0.02071(0.99)\n",
      "Batchnorm Loss(Acc): 0.02071(0.99), TP: 7123, TN: 11133, FP, 50, FN: 46\n",
      "\n",
      "[Epoch 20-TRAIN] Batchnorm Loss(Acc): 0.01115(1.00)\n",
      "Batchnorm Loss(Acc): 0.01115(1.00), TP: 134204, TN: 213283, FP, 426, FN: 763\n",
      "[Epoch 20-VALID] Batchnorm Loss(Acc): 0.02070(1.00)\n",
      "Batchnorm Loss(Acc): 0.02070(1.00), TP: 7115, TN: 11153, FP, 30, FN: 54\n",
      "\n",
      "[Epoch 21-TRAIN] Batchnorm Loss(Acc): 0.03168(0.99)\n",
      "Batchnorm Loss(Acc): 0.03168(0.99), TP: 131146, TN: 213484, FP, 225, FN: 3821\n",
      "[Epoch 21-VALID] Batchnorm Loss(Acc): 0.04009(0.99)\n",
      "Batchnorm Loss(Acc): 0.04009(0.99), TP: 6952, TN: 11167, FP, 16, FN: 217\n",
      "\n",
      "[Epoch 22-TRAIN] Batchnorm Loss(Acc): 0.01904(0.99)\n",
      "Batchnorm Loss(Acc): 0.01904(0.99), TP: 133349, TN: 213433, FP, 276, FN: 1618\n",
      "[Epoch 22-VALID] Batchnorm Loss(Acc): 0.02288(0.99)\n",
      "Batchnorm Loss(Acc): 0.02288(0.99), TP: 7075, TN: 11161, FP, 22, FN: 94\n",
      "\n",
      "[Epoch 23-TRAIN] Batchnorm Loss(Acc): 0.01119(1.00)\n",
      "Batchnorm Loss(Acc): 0.01119(1.00), TP: 134446, TN: 213109, FP, 600, FN: 521\n",
      "[Epoch 23-VALID] Batchnorm Loss(Acc): 0.01867(1.00)\n",
      "Batchnorm Loss(Acc): 0.01867(1.00), TP: 7123, TN: 11142, FP, 41, FN: 46\n",
      "\n",
      "[Epoch 24-TRAIN] Batchnorm Loss(Acc): 0.01133(1.00)\n",
      "Batchnorm Loss(Acc): 0.01133(1.00), TP: 133898, TN: 213438, FP, 271, FN: 1069\n",
      "[Epoch 24-VALID] Batchnorm Loss(Acc): 0.02125(1.00)\n",
      "Batchnorm Loss(Acc): 0.02125(1.00), TP: 7102, TN: 11163, FP, 20, FN: 67\n",
      "\n",
      "[Epoch 25-TRAIN] Batchnorm Loss(Acc): 0.01164(1.00)\n",
      "Batchnorm Loss(Acc): 0.01164(1.00), TP: 134518, TN: 212953, FP, 756, FN: 449\n",
      "[Epoch 25-VALID] Batchnorm Loss(Acc): 0.02337(0.99)\n",
      "Batchnorm Loss(Acc): 0.02337(0.99), TP: 7131, TN: 11129, FP, 54, FN: 38\n",
      "\n",
      "[Epoch 26-TRAIN] Batchnorm Loss(Acc): 0.01161(1.00)\n",
      "Batchnorm Loss(Acc): 0.01161(1.00), TP: 134587, TN: 212948, FP, 761, FN: 380\n",
      "[Epoch 26-VALID] Batchnorm Loss(Acc): 0.02448(0.99)\n",
      "Batchnorm Loss(Acc): 0.02448(0.99), TP: 7130, TN: 11125, FP, 58, FN: 39\n",
      "\n",
      "[Epoch 27-TRAIN] Batchnorm Loss(Acc): 0.01464(1.00)\n",
      "Batchnorm Loss(Acc): 0.01464(1.00), TP: 134734, TN: 212506, FP, 1203, FN: 233\n",
      "[Epoch 27-VALID] Batchnorm Loss(Acc): 0.02650(0.99)\n",
      "Batchnorm Loss(Acc): 0.02650(0.99), TP: 7144, TN: 11102, FP, 81, FN: 25\n",
      "\n",
      "[Epoch 28-TRAIN] Batchnorm Loss(Acc): 0.01038(1.00)\n",
      "Batchnorm Loss(Acc): 0.01038(1.00), TP: 134322, TN: 213325, FP, 384, FN: 645\n",
      "[Epoch 28-VALID] Batchnorm Loss(Acc): 0.02344(1.00)\n",
      "Batchnorm Loss(Acc): 0.02344(1.00), TP: 7120, TN: 11154, FP, 29, FN: 49\n",
      "\n",
      "[Epoch 29-TRAIN] Batchnorm Loss(Acc): 0.00919(1.00)\n",
      "Batchnorm Loss(Acc): 0.00919(1.00), TP: 134571, TN: 213070, FP, 639, FN: 396\n",
      "[Epoch 29-VALID] Batchnorm Loss(Acc): 0.02170(1.00)\n",
      "Batchnorm Loss(Acc): 0.02170(1.00), TP: 7124, TN: 11139, FP, 44, FN: 45\n",
      "\n",
      "[Epoch 30-TRAIN] Batchnorm Loss(Acc): 0.01657(0.99)\n",
      "Batchnorm Loss(Acc): 0.01657(0.99), TP: 133611, TN: 213075, FP, 634, FN: 1356\n",
      "[Epoch 30-VALID] Batchnorm Loss(Acc): 0.03452(0.99)\n",
      "Batchnorm Loss(Acc): 0.03452(0.99), TP: 7064, TN: 11130, FP, 53, FN: 105\n",
      "\n",
      "[Epoch 31-TRAIN] Batchnorm Loss(Acc): 0.01806(1.00)\n",
      "Batchnorm Loss(Acc): 0.01806(1.00), TP: 134723, TN: 212214, FP, 1495, FN: 244\n",
      "[Epoch 31-VALID] Batchnorm Loss(Acc): 0.03268(0.99)\n",
      "Batchnorm Loss(Acc): 0.03268(0.99), TP: 7141, TN: 11089, FP, 94, FN: 28\n",
      "\n",
      "[Epoch 32-TRAIN] Batchnorm Loss(Acc): 0.01305(1.00)\n",
      "Batchnorm Loss(Acc): 0.01305(1.00), TP: 134531, TN: 212862, FP, 847, FN: 436\n",
      "[Epoch 32-VALID] Batchnorm Loss(Acc): 0.02524(0.99)\n",
      "Batchnorm Loss(Acc): 0.02524(0.99), TP: 7132, TN: 11125, FP, 58, FN: 37\n",
      "\n",
      "[Epoch 33-TRAIN] Batchnorm Loss(Acc): 0.01705(1.00)\n",
      "Batchnorm Loss(Acc): 0.01705(1.00), TP: 134686, TN: 212260, FP, 1449, FN: 281\n",
      "[Epoch 33-VALID] Batchnorm Loss(Acc): 0.03200(0.99)\n",
      "Batchnorm Loss(Acc): 0.03200(0.99), TP: 7143, TN: 11093, FP, 90, FN: 26\n",
      "\n",
      "[Epoch 34-TRAIN] Batchnorm Loss(Acc): 0.01162(1.00)\n",
      "Batchnorm Loss(Acc): 0.01162(1.00), TP: 134731, TN: 212656, FP, 1053, FN: 236\n",
      "[Epoch 34-VALID] Batchnorm Loss(Acc): 0.02425(0.99)\n",
      "Batchnorm Loss(Acc): 0.02425(0.99), TP: 7143, TN: 11112, FP, 71, FN: 26\n",
      "\n",
      "[Epoch 35-TRAIN] Batchnorm Loss(Acc): 0.00973(1.00)\n",
      "Batchnorm Loss(Acc): 0.00973(1.00), TP: 134680, TN: 213056, FP, 653, FN: 287\n",
      "[Epoch 35-VALID] Batchnorm Loss(Acc): 0.02471(1.00)\n",
      "Batchnorm Loss(Acc): 0.02471(1.00), TP: 7134, TN: 11136, FP, 47, FN: 35\n",
      "\n",
      "[Epoch 36-TRAIN] Batchnorm Loss(Acc): 0.02591(0.99)\n",
      "Batchnorm Loss(Acc): 0.02591(0.99), TP: 132077, TN: 213061, FP, 648, FN: 2890\n",
      "[Epoch 36-VALID] Batchnorm Loss(Acc): 0.03494(0.99)\n",
      "Batchnorm Loss(Acc): 0.03494(0.99), TP: 6999, TN: 11138, FP, 45, FN: 170\n",
      "\n",
      "[Epoch 37-TRAIN] Batchnorm Loss(Acc): 0.01313(1.00)\n",
      "Batchnorm Loss(Acc): 0.01313(1.00), TP: 134764, TN: 212537, FP, 1172, FN: 203\n",
      "[Epoch 37-VALID] Batchnorm Loss(Acc): 0.02611(0.99)\n",
      "Batchnorm Loss(Acc): 0.02611(0.99), TP: 7143, TN: 11096, FP, 87, FN: 26\n",
      "\n",
      "[Epoch 38-TRAIN] Batchnorm Loss(Acc): 0.01557(0.99)\n",
      "Batchnorm Loss(Acc): 0.01557(0.99), TP: 134550, TN: 212373, FP, 1336, FN: 417\n",
      "[Epoch 38-VALID] Batchnorm Loss(Acc): 0.03122(0.99)\n",
      "Batchnorm Loss(Acc): 0.03122(0.99), TP: 7136, TN: 11098, FP, 85, FN: 33\n",
      "\n",
      "[Epoch 39-TRAIN] Batchnorm Loss(Acc): 0.01644(1.00)\n",
      "Batchnorm Loss(Acc): 0.01644(1.00), TP: 134821, TN: 212206, FP, 1503, FN: 146\n",
      "[Epoch 39-VALID] Batchnorm Loss(Acc): 0.03385(0.99)\n",
      "Batchnorm Loss(Acc): 0.03385(0.99), TP: 7147, TN: 11073, FP, 110, FN: 22\n",
      "\n",
      "[Epoch 40-TRAIN] Batchnorm Loss(Acc): 0.00917(1.00)\n",
      "Batchnorm Loss(Acc): 0.00917(1.00), TP: 134538, TN: 213123, FP, 586, FN: 429\n",
      "[Epoch 40-VALID] Batchnorm Loss(Acc): 0.02057(1.00)\n",
      "Batchnorm Loss(Acc): 0.02057(1.00), TP: 7136, TN: 11137, FP, 46, FN: 33\n",
      "\n",
      "[Epoch 41-TRAIN] Batchnorm Loss(Acc): 0.01109(1.00)\n",
      "Batchnorm Loss(Acc): 0.01109(1.00), TP: 134710, TN: 212739, FP, 970, FN: 257\n",
      "[Epoch 41-VALID] Batchnorm Loss(Acc): 0.02893(0.99)\n",
      "Batchnorm Loss(Acc): 0.02893(0.99), TP: 7137, TN: 11116, FP, 67, FN: 32\n",
      "\n",
      "[Epoch 42-TRAIN] Batchnorm Loss(Acc): 0.01214(1.00)\n",
      "Batchnorm Loss(Acc): 0.01214(1.00), TP: 134634, TN: 212881, FP, 828, FN: 333\n",
      "[Epoch 42-VALID] Batchnorm Loss(Acc): 0.02795(0.99)\n",
      "Batchnorm Loss(Acc): 0.02795(0.99), TP: 7131, TN: 11126, FP, 57, FN: 38\n",
      "\n",
      "[Epoch 43-TRAIN] Batchnorm Loss(Acc): 0.02394(0.99)\n",
      "Batchnorm Loss(Acc): 0.02394(0.99), TP: 134775, TN: 211731, FP, 1978, FN: 192\n",
      "[Epoch 43-VALID] Batchnorm Loss(Acc): 0.04038(0.99)\n",
      "Batchnorm Loss(Acc): 0.04038(0.99), TP: 7144, TN: 11055, FP, 128, FN: 25\n",
      "\n",
      "[Epoch 44-TRAIN] Batchnorm Loss(Acc): 0.01693(0.99)\n",
      "Batchnorm Loss(Acc): 0.01693(0.99), TP: 132413, TN: 213399, FP, 310, FN: 2554\n",
      "[Epoch 44-VALID] Batchnorm Loss(Acc): 0.03203(0.99)\n",
      "Batchnorm Loss(Acc): 0.03203(0.99), TP: 7011, TN: 11159, FP, 24, FN: 158\n",
      "\n",
      "[Epoch 45-TRAIN] Batchnorm Loss(Acc): 0.00764(1.00)\n",
      "Batchnorm Loss(Acc): 0.00764(1.00), TP: 134688, TN: 213193, FP, 516, FN: 279\n",
      "[Epoch 45-VALID] Batchnorm Loss(Acc): 0.02094(1.00)\n",
      "Batchnorm Loss(Acc): 0.02094(1.00), TP: 7134, TN: 11143, FP, 40, FN: 35\n",
      "\n",
      "[Epoch 46-TRAIN] Batchnorm Loss(Acc): 0.01134(1.00)\n",
      "Batchnorm Loss(Acc): 0.01134(1.00), TP: 134792, TN: 212781, FP, 928, FN: 175\n",
      "[Epoch 46-VALID] Batchnorm Loss(Acc): 0.02737(0.99)\n",
      "Batchnorm Loss(Acc): 0.02737(0.99), TP: 7145, TN: 11113, FP, 70, FN: 24\n",
      "\n",
      "[Epoch 47-TRAIN] Batchnorm Loss(Acc): 0.01211(1.00)\n",
      "Batchnorm Loss(Acc): 0.01211(1.00), TP: 134045, TN: 213266, FP, 443, FN: 922\n",
      "[Epoch 47-VALID] Batchnorm Loss(Acc): 0.02797(0.99)\n",
      "Batchnorm Loss(Acc): 0.02797(0.99), TP: 7099, TN: 11151, FP, 32, FN: 70\n",
      "\n",
      "[Epoch 48-TRAIN] Batchnorm Loss(Acc): 0.02298(0.99)\n",
      "Batchnorm Loss(Acc): 0.02298(0.99), TP: 134758, TN: 212001, FP, 1708, FN: 209\n",
      "[Epoch 48-VALID] Batchnorm Loss(Acc): 0.04151(0.99)\n",
      "Batchnorm Loss(Acc): 0.04151(0.99), TP: 7143, TN: 11065, FP, 118, FN: 26\n",
      "\n",
      "[Epoch 49-TRAIN] Batchnorm Loss(Acc): 0.00768(1.00)\n",
      "Batchnorm Loss(Acc): 0.00768(1.00), TP: 134420, TN: 213344, FP, 365, FN: 547\n",
      "[Epoch 49-VALID] Batchnorm Loss(Acc): 0.02290(1.00)\n",
      "Batchnorm Loss(Acc): 0.02290(1.00), TP: 7129, TN: 11155, FP, 28, FN: 40\n",
      "\n",
      "[Epoch 50-TRAIN] Batchnorm Loss(Acc): 0.01146(1.00)\n",
      "Batchnorm Loss(Acc): 0.01146(1.00), TP: 134607, TN: 212929, FP, 780, FN: 360\n",
      "[Epoch 50-VALID] Batchnorm Loss(Acc): 0.02655(0.99)\n",
      "Batchnorm Loss(Acc): 0.02655(0.99), TP: 7136, TN: 11124, FP, 59, FN: 33\n",
      "\n",
      "[Epoch 51-TRAIN] Batchnorm Loss(Acc): 0.00935(1.00)\n",
      "Batchnorm Loss(Acc): 0.00935(1.00), TP: 134661, TN: 213086, FP, 623, FN: 306\n",
      "[Epoch 51-VALID] Batchnorm Loss(Acc): 0.02535(1.00)\n",
      "Batchnorm Loss(Acc): 0.02535(1.00), TP: 7139, TN: 11135, FP, 48, FN: 30\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4c309499bc08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#_, nn_loss = nn_solver.train(X_batch, y_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-150f9659ae82>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pika/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    ''' ##not use batch\n",
    "    for _ in range(N//batch_size):\n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _, bn_loss = bn_solver.train(X_batch, y_batch)\n",
    "        _, nn_loss = nn_solver.train(X_batch, y_batch)       \n",
    "    '''\n",
    "\n",
    "    for inds in minibatches[:-1]:\n",
    "        # print(\"\\rbatch %d / %d\"%(b_ind, n_batches), end=\"\")\n",
    "        # sys.stdout.flush()\n",
    "        X_batch = X_train[inds, :]\n",
    "        y_batch = y_train[inds]\n",
    "        _, bn_loss = bn_solver.train(X_batch, y_batch)\n",
    "        #_, nn_loss = nn_solver.train(X_batch, y_batch)\n",
    "    \n",
    "    \n",
    "    ##b_loss, b_acc = bn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
    "    b_loss, b_acc, b_tp, b_tn, b_fp, b_fn = bn_solver.evaluate(X_train, y_train, batch_size)\n",
    "    ##n_loss, n_acc = nn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
    "    #n_loss, n_acc, n_tp, n_tn, n_fp, n_fn = nn_solver.evaluate(X_train, y_train, batch_size)\n",
    "    \n",
    "    # Save train losses/acc\n",
    "    '''\n",
    "    train_losses.append([b_loss, n_loss])\n",
    "    train_accs.append([b_acc, n_acc])\n",
    "    train_ROC.append([[b_tp, b_tn, b_fp, b_fn],[n_tp, n_tn, n_fp, n_fn]])\n",
    "    # print(f'[Epoch {epoch}-TRAIN] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n",
    "    print('[Epoch %d-TRAIN] Batchnorm Loss(Acc): %.5f(%.2f) vs No Batchnorm Loss(Acc): %.5f(%.2f)' % (epoch, b_loss, b_acc, n_loss, n_acc))\n",
    "    print('Batchnorm Loss(Acc): %.5f(%.2f), TP: %d, TN: %d, FP, %d, FN: %d' % (b_loss, b_acc,b_tp, b_tn, b_fp, b_fn))\n",
    "    print('No Batchnorm Loss(Acc): %.5f(%.2f), TP: %d, TN: %d, FP, %d, FN: %d' % (n_loss, n_acc, n_tp, n_tn, n_fp, n_fn))\n",
    "    '''\n",
    "    train_losses.append(b_loss)\n",
    "    train_accs.append(b_acc)\n",
    "    train_ROC.append([b_tp, b_tn, b_fp, b_fn])\n",
    "    # print(f'[Epoch {epoch}-TRAIN] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n",
    "    print('[Epoch %d-TRAIN] Batchnorm Loss(Acc): %.5f(%.2f)' % (epoch, b_loss, b_acc))\n",
    "    print('Batchnorm Loss(Acc): %.5f(%.2f), TP: %d, TN: %d, FP, %d, FN: %d' % (b_loss, b_acc,b_tp, b_tn, b_fp, b_fn))\n",
    "    # print('No Batchnorm Loss(Acc): %.5f(%.2f), TP: %d, TN: %d, FP, %d, FN: %d' % (n_loss, n_acc, n_tp, n_tn, n_fp, n_fn))\n",
    "    \n",
    "    ##b_loss, b_acc = bn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
    "    b_loss, b_acc, b_tp, b_tn, b_fp, b_fn = bn_solver.evaluate(X_valid, y_valid)\n",
    "    ##n_loss, n_acc = nn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
    "    #n_loss, n_acc, n_tp, n_tn, n_fp, n_fn = nn_solver.evaluate(X_valid, y_valid)\n",
    "    \n",
    "    # Save valid losses/acc\n",
    "    '''\n",
    "    valid_losses.append([b_loss, n_loss])\n",
    "    valid_accs.append([b_acc, n_acc])\n",
    "    valid_ROC.append([[b_tp, b_tn, b_fp, b_fn],[n_tp, n_tn, n_fp, n_fn]])\n",
    "    print(f'[Epoch {epoch}-VALID] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n",
    "    print('[Epoch %d-VALID] Batchnorm Loss(Acc): %.5f(%.2f) vs No Batchnorm Loss(Acc): %.5f(%.2f)' % (epoch, b_loss, b_acc, n_loss, n_acc))\n",
    "    print('Batchnorm Loss(Acc): %.5f(%.2f), TP: %d, TN: %d, FP, %d, FN: %d' % (b_loss, b_acc,b_tp, b_tn, b_fp, b_fn))\n",
    "    print('No Batchnorm Loss(Acc): %.5f(%.2f), TP: %d, TN: %d, FP, %d, FN: %d' % (n_loss, n_acc, n_tp, n_tn, n_fp, n_fn))\n",
    "    '''\n",
    "    valid_losses.append(b_loss)\n",
    "    valid_accs.append(b_acc)\n",
    "    valid_ROC.append([b_tp, b_tn, b_fp, b_fn])\n",
    "    # print(f'[Epoch {epoch}-TRAIN] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n",
    "    print('[Epoch %d-VALID] Batchnorm Loss(Acc): %.5f(%.2f)' % (epoch, b_loss, b_acc))\n",
    "    print('Batchnorm Loss(Acc): %.5f(%.2f), TP: %d, TN: %d, FP, %d, FN: %d' % (b_loss, b_acc,b_tp, b_tn, b_fp, b_fn))\n",
    "    # print('No Batchnorm Loss(Acc): %.5f(%.2f), TP: %d, TN: %d, FP, %d, FN: %d' % (n_loss, n_acc, n_tp, n_tn, n_fp, n_fn))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206762, 772)\n",
      "(206762, 1)\n"
     ]
    }
   ],
   "source": [
    "# load test\n",
    "\n",
    "test_malicious = np.genfromtxt(\"./JSF20170324/test/malicious.vlog\", delimiter=',')\n",
    "test_normal    = np.genfromtxt(\"./JSF20170324/test/normal.vlog\"   , delimiter=',')\n",
    "\n",
    "X_test = np.concatenate((test_malicious, test_normal))\n",
    "y_test = np.array([1]*len(test_malicious) + [0]*len(test_normal))\n",
    "del(test_malicious)\n",
    "del(test_normal)\n",
    "y_test = np.reshape(y_test, [len(y_test), 1])\n",
    "X_test = np.delete(X_test, np.s_[:1], axis=1)\n",
    "X_test = np.log(X_test+1)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b_loss, b_acc, b_tp, b_tn, b_fp, b_fn = bn_solver.evaluate(X_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.312768378784 0.963465295507 86636.0 112572.0 594.0 6960.0\n"
     ]
    }
   ],
   "source": [
    "print(b_loss, b_acc, b_tp, b_tn, b_fp, b_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nn_solver.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_compare(loss_list: list, ylim=None, title=None) -> None:\n",
    "    \n",
    "    bn = [i[0] for i in loss_list]\n",
    "    nn = [i[1] for i in loss_list]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(bn, label='With BN')\n",
    "    plt.plot(nn, label='Without BN')\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "        \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid('on')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_compare(train_losses, title='Training Loss at Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_compare(train_accs, [0, 1.0], title=\"Training Acc at Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_compare(valid_losses, title='Validation Loss at Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plot_compare(valid_accs, [0, 1.], title='Validation Acc at Epoch')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
