{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Model for Binary Classification\n",
    "---\n",
    "- tensorboard include {scalars, histogram and embedding}\n",
    "\n",
    "## Reference\n",
    "- [basic tensorboard usage](https://github.com/mamcgrath/TensorBoard-TF-Dev-Summit-Tutorial/blob/master/mnist.py)\n",
    "- [Great example with multiple embedding](https://github.com/tensorflow/tensorflow/issues/6322)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extract \n",
    "---\n",
    "- Input feature is the unigram model extrract by ATSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(367028, 772)\n",
      "(367028, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_malicious = np.genfromtxt(\"./JSF20170324/malicious.vlog\", delimiter=',')\n",
    "train_normal    = np.genfromtxt(\"./JSF20170324/normal.vlog\"   , delimiter=',')\n",
    "\n",
    "X = np.concatenate((train_malicious, train_normal))\n",
    "y = np.array([1]*len(train_malicious) + [0]*len(train_normal))\n",
    "del(train_malicious)\n",
    "del(train_normal)\n",
    "y = np.reshape(y, [len(y), 1])\n",
    "X = np.delete(X, np.s_[:1], axis=1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348676, 772)\n",
      "(18352, 772)\n",
      "(348676, 1)\n",
      "(18352, 1)\n"
     ]
    }
   ],
   "source": [
    "## Split Part of training data as validation set \n",
    "## Size of data is big enought, no need to split k-fold for cross validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=831\n",
    ")\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "---\n",
    "- here we just log done the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = np.log(X_train+1)\n",
    "X_valid = np.log(X_valid+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batch :  697\n",
      "Total traning data:  348676\n",
      "Batch size : 500\n"
     ]
    }
   ],
   "source": [
    "## Dimension Setting, (if multi classification, remember to choose loss with softmax)\n",
    "input_dim = 772\n",
    "output_dim = 1\n",
    "N = len(X_train)\n",
    "batch_size = 500\n",
    "\n",
    "## Get Mini Batch\n",
    "print(\"Number of batch : \", N//batch_size)\n",
    "print(\"Total traning data: \", N)\n",
    "print(\"Batch size :\", batch_size)\n",
    "\n",
    "def get_minibatches_index(n, batch_size, shuffle=False):\n",
    "    \"\"\"\n",
    "    Used to shuffle the dataset at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    idx_list = np.arange(n, dtype=\"int32\")\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "\n",
    "    minibatches = []\n",
    "    minibatch_start = 0\n",
    "    for i in range(n // batch_size):\n",
    "        minibatches.append(idx_list[minibatch_start:\n",
    "                                        minibatch_start + batch_size])\n",
    "        minibatch_start += batch_size\n",
    "\n",
    "    if (minibatch_start != n):\n",
    "        # Make a minibatch out of what is left\n",
    "        minibatches.append(idx_list[minibatch_start:])\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "minibatches = get_minibatches_index(len(X_train), batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Solver class\n",
    "---\n",
    "- if use the momentum as optimizer, just set the `momentum` parameter\n",
    "- dense = activcation(inputs.kernel+bias), kernel = w*x\n",
    "- [softmax and with cross entropy](http://stackoverflow.com/questions/34240703/difference-between-tensorflow-tf-nn-softmax-and-tf-nn-softmax-cross-entropy-with)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, name, input_dim, output_dim, hidden_dims=[32, 32], use_batchnorm=True, \\\n",
    "                 activation_fn=tf.nn.relu, optimizer=tf.train.AdamOptimizer, lr=0.01, momentum=None):\n",
    "        \"\"\" Constructor\"\"\"\n",
    "        self.hidden_dims    = hidden_dims\n",
    "        self.name           = name\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            with tf.name_scope('input'):\n",
    "                self.X = tf.placeholder(tf.float32, [None, input_dim], name='x')\n",
    "                self.y = tf.placeholder(tf.float32, [None, output_dim], name='labels')\n",
    "                self.mode = tf.placeholder(tf.bool, name='train_mode')            \n",
    "            \n",
    "            # Loop over hidden layers\n",
    "            net = self.X\n",
    "            for i, h_dim in enumerate(hidden_dims):\n",
    "                with tf.variable_scope('layer{}'.format(i)):\n",
    "                    net = tf.layers.dense(net, h_dim, name=\"fc{}\".format(i))\n",
    "                    if use_batchnorm:\n",
    "                        net = tf.layers.batch_normalization(net, training=self.mode)\n",
    "                    net = activation_fn(net, name=\"afc{}\".format(i))\n",
    "                    \n",
    "            net = tf.contrib.layers.flatten(net)\n",
    "            net = tf.layers.dense(net, output_dim, name='last_fc')\n",
    "            tf.summary.histogram('last_fc',net)\n",
    "            \n",
    "            with tf.name_scope('loss'):\n",
    "                # (for multiclass) self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=self.y)\n",
    "                self.loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=net, labels=self.y)\n",
    "                self.loss = tf.reduce_mean(self.loss, name='loss')    \n",
    "\n",
    "            with tf.name_scope('train'):\n",
    "                # When using the batchnormalization layers,\n",
    "                # it is necessary to manually add the update operations\n",
    "                # because the moving averages are not included in the graph            \n",
    "                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)\n",
    "                with tf.control_dependencies(update_ops):\n",
    "                    if momentum is not None:\n",
    "                        self.train_op = tf.train.MomentumOptimizer(lr,momentum).minimize(self.loss)\n",
    "                    else:\n",
    "                        self.train_op = optimizer(lr).minimize(self.loss)\n",
    "            \n",
    "            with tf.name_scope('accuracy'):\n",
    "                self.predicted = tf.cast(net > 0.5, dtype=tf.float32)\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predicted, self.y), dtype=tf.float32))\n",
    "            \n",
    "            with tf.name_scope('roc'):\n",
    "                predi_bool = tf.cast(self.predicted, tf.bool)\n",
    "                label_bool = tf.cast(self.y, tf.bool)\n",
    "                # for TP\n",
    "                TP_and = tf.logical_and(predi_bool, label_bool)\n",
    "                # for TN\n",
    "                tmp_or = tf.logical_or(predi_bool, label_bool)\n",
    "                TN_not = tf.logical_not(tmp_or)\n",
    "                # for FP\n",
    "                label_not = tf.logical_not(label_bool)\n",
    "                FP_and = tf.logical_and(predi_bool, label_not)\n",
    "                # for FN\n",
    "                predi_not = tf.logical_not(predi_bool)\n",
    "                FN_and = tf.logical_and(predi_not, label_bool)\n",
    "                # Add node\n",
    "                self.TP = tf.reduce_sum(tf.cast(TP_and, tf.float32))\n",
    "                self.TN = tf.reduce_sum(tf.cast(TN_not, tf.float32))\n",
    "                self.FP = tf.reduce_sum(tf.cast(FP_and, tf.float32))\n",
    "                self.FN = tf.reduce_sum(tf.cast(FN_and, tf.float32))\n",
    "            \n",
    "                self.precision = self.TP / (self.TP + self.FP)\n",
    "                self.recall    = self.TP / (self.TP + self.FN)\n",
    "            \n",
    "            # Create Summary\n",
    "            for i, h_dim in enumerate(hidden_dims):\n",
    "                net = tf.get_default_graph().get_tensor_by_name(self.name+'/layer'+str(i)+'/fc'+str(i)+'/MatMul:0')\n",
    "                tf.summary.histogram('W_layer{}'.format(i), net)\n",
    "            \n",
    "            tf.summary.scalar(\"cosr\", self.loss)\n",
    "            tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "            tf.summary.scalar(\"precision\", self.precision)\n",
    "            tf.summary.scalar(\"recall\", self.recall)\n",
    "           \n",
    "            self.merged = tf.summary.merge_all()\n",
    "            \n",
    "            # construct Saver\n",
    "            self.saver = tf.train.Saver()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, sess, model, log_dir):\n",
    "        self.model = model\n",
    "        self.sess = sess\n",
    "        self.epoch = 0\n",
    "        self.iteration = 0\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        train_path = os.path.join(self.log_dir, self.model.name+'_train')\n",
    "        self.train_writer = tf.summary.FileWriter(train_path, sess.graph)\n",
    "        \n",
    "        valid_path = os.path.join(self.log_dir, self.model.name+'_valid')\n",
    "        self.valid_writer  = tf.summary.FileWriter(valid_path, sess.graph)\n",
    "        \n",
    "    def save(self, epoch):\n",
    "        self.model.saver.save(self.sess, os.path.join(self.log_dir, \"model[\"+self.model.name+\"].ckpt\"), epoch)\n",
    "        \n",
    "    def compute_embedding(self, X, y,name):\n",
    "        ''' name could be train, validation or test ... '''\n",
    "        log_dir = self.log_dir\n",
    "        feed = {\n",
    "            self.model.X: X,\n",
    "            self.model.y: y,\n",
    "            self.model.mode: True\n",
    "        }\n",
    "        number_of_embed_watch = len(X)\n",
    "        accuracy = self.model.accuracy\n",
    "        output_path = os.path.join(log_dir,self.model.name+'_embed', name)\n",
    "        embed_tensors = []\n",
    "        embed_writer = tf.summary.FileWriter(output_path, sess.graph)\n",
    "        config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "        hidden_dims = self.model.hidden_dims\n",
    "        hidden_outputs = list()\n",
    "\n",
    "        for i in range(len(hidden_dims)):\n",
    "            hidden_outputs.append(tf.get_default_graph().get_tensor_by_name(self.model.name+'/layer'+str(i)+'/fc'+str(i)+'/MatMul:0'))\n",
    "        hidden_outputs.append(accuracy)\n",
    "        hidden_vectors = sess.run(hidden_outputs, feed_dict=feed)\n",
    "        for layer, embed_vectors in enumerate(hidden_vectors[:-1]):\n",
    "            embed_tensor = tf.Variable(tf.zeros([number_of_embed_watch, hidden_dims[layer]]), \n",
    "                                       name=\"%s_layer_%s\" % (name, layer))\n",
    "            embed_tensor_assign = embed_tensor.assign(embed_vectors)\n",
    "            embed_tensors.append(embed_tensor)\n",
    "            sess.run(embed_tensor_assign)\n",
    "            embedding = config.embeddings.add()\n",
    "            embedding.tensor_name = embed_tensor.name\n",
    "            embedding.metadata_path = os.path.join(output_path, self.model.name+'_labels.tsv')\n",
    "            tf.contrib.tensorboard.plugins.projector.visualize_embeddings(embed_writer, config)\n",
    "        result = sess.run(embed_tensors)\n",
    "        saver  = tf.train.Saver(embed_tensors)\n",
    "        # this must be named `model.ckpt`\n",
    "        saver.save(sess, os.path.join(output_path, 'model.ckpt'), layer)\n",
    "        \n",
    "        # Make Label \n",
    "        class_literal=[\"NORMAL\", \"MALICIOUS\"]\n",
    "        with open(os.path.join(output_path, self.model.name+'_labels.tsv'), 'w') as metadata_file:\n",
    "            metadata_file .write('Name\\tClass\\n')\n",
    "            for l in range(len(y)):\n",
    "                metadata_file.write('%d\\t%s\\n' % (y[l], class_literal[int(y[l])]))\n",
    "                \n",
    "                \n",
    "    def train(self, X, y):\n",
    "        self.iteration += 1\n",
    "        merged = self.model.merged\n",
    "        loss = self.model.loss\n",
    "        accuracy = self.model.accuracy\n",
    "        TP = self.model.TP\n",
    "        TN = self.model.TN\n",
    "        FP = self.model.FP\n",
    "        FN = self.model.FN\n",
    "        feed = {\n",
    "            self.model.X: X,\n",
    "            self.model.y: y,\n",
    "            self.model.mode: True\n",
    "        }\n",
    "        train_op = self.model.train_op\n",
    "        loss = self.model.loss\n",
    "        if self.iteration % 5 == 0:\n",
    "            summary, itr_loss, itr_acc, itr_tp, itr_tn, itr_fp, ite_fn = \\\n",
    "                self.sess.run([merged, loss, accuracy, TP, TN, FP, FN], feed_dict=feed)\n",
    "            self.train_writer.add_summary(summary, self.iteration)\n",
    "            \n",
    "        itr_train_op, itr_loss = self.sess.run([train_op, loss], feed_dict=feed)\n",
    "        \n",
    "        return (itr_train_op, itr_loss)\n",
    "    \n",
    "    def evaluate(self, X, y, batch_size=None):\n",
    "        loss = self.model.loss\n",
    "        accuracy = self.model.accuracy\n",
    "        TP = self.model.TP\n",
    "        TN = self.model.TN\n",
    "        FP = self.model.FP\n",
    "        FN = self.model.FN\n",
    "        '''\n",
    "        The x-axis of the plots below are always in units of epochs, \n",
    "        which measure how many times every example has been seen during training in expectation \n",
    "        (e.g. one epoch means that every example has been seen once). \n",
    "        It is preferable to track epochs rather than iterations since the number of iterations\n",
    "        depends on the arbitrary setting of batch size.\n",
    "        '''\n",
    "        if batch_size:\n",
    "            N = X.shape[0] \n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            total_TP = 0\n",
    "            total_TN = 0\n",
    "            total_FP = 0\n",
    "            total_FN = 0\n",
    "            for i in range(0, N, batch_size):\n",
    "                # Siple Sequencial Batch\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                feed = {\n",
    "                    self.model.X: X_batch,\n",
    "                    self.model.y: y_batch,\n",
    "                    self.model.mode: False\n",
    "                }\n",
    "                step_loss, step_acc, step_tp, step_tn, step_fp, step_fn = \\\n",
    "                    self.sess.run([loss, accuracy, TP, TN, FP, FN], feed_dict=feed)\n",
    "                \n",
    "                total_loss += step_loss * X_batch.shape[0]\n",
    "                total_acc += step_acc * X_batch.shape[0]\n",
    "                total_TP += step_tp\n",
    "                total_TN += step_tn\n",
    "                total_FP += step_fp\n",
    "                total_FN += step_fn\n",
    "                \n",
    "                # self.writer.add_summary(summary, i) # Batch \n",
    "            total_loss /= N\n",
    "            total_acc /= N\n",
    "            return (total_loss, total_acc, total_TP, total_TN, total_FP, total_FN)\n",
    "            \n",
    "        else:\n",
    "            merged = self.model.merged\n",
    "            feed = {\n",
    "                self.model.X: X,\n",
    "                self.model.y: y,\n",
    "                self.model.mode: False\n",
    "            }\n",
    "            \n",
    "            loss = self.model.loss            \n",
    "            accuracy = self.model.accuracy\n",
    "            TP = self.model.TP\n",
    "            TN = self.model.TN\n",
    "            FP = self.model.FP\n",
    "            FN = self.model.FN\n",
    "            epoch_summary, total_loss, total_acc, total_TP, total_TN, total_FP, total_FN = \\\n",
    "                self.sess.run([merged, loss, accuracy, TP, TN, FP, FN], feed_dict=feed)\n",
    "            self.valid_writer.add_summary(epoch_summary, self.epoch)\n",
    "            \n",
    "            return (total_loss, total_acc, total_TP, total_TN, total_FP, total_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create model and solver\n",
    "---\n",
    "- useadamOptimizer ..\n",
    "\n",
    "```Model('dnnAd', input_dim, output_dim, hidden_dims=[1024, 1024, 1024, 1024], use_batchnorm=True, lr=0.1)```\n",
    "\n",
    "- use momentum\n",
    "\n",
    "```Model('dnnMo', input_dim, output_dim, hidden_dims=[1024, 1024, 1024, 1024], use_batchnorm=True, lr=0.1, momentum=0.5)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Here Should be the Main Function \n",
    "\n",
    "# Epoch Setting \n",
    "epoch_n = 50\n",
    "\n",
    "# Clean graph and Creat Session \n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "log_dir = \"./file_writer_dnn/\"\n",
    "if tf.gfile.Exists(log_dir):\n",
    "    tf.gfile.DeleteRecursively(log_dir)\n",
    "tf.gfile.MakeDirs(log_dir)\n",
    "\n",
    "\n",
    "# Create Graph \n",
    "bn = Model('dnnMo', input_dim, output_dim, hidden_dims=[1024, 1024, 1024, 1024], \\\n",
    "           use_batchnorm=True, lr=0.1, momentum=0.5)\n",
    "\n",
    "\n",
    "# Create Train Module \n",
    "bn_solver = Solver(sess, bn, log_dir)\n",
    "\n",
    "# Save for record\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "train_ROC = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "valid_ROC = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train\n",
    "`tensorboard --logdir=\"./file_writer_dnn/\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0-TRAIN] Batchnorm Loss(Acc): 0.01906(0.99)\n",
      "Batchnorm Loss(Acc): 0.01906(0.99), TP: 132407, TN: 213296, FP, 413, FN: 2560\n",
      "[Epoch 0-VALID] Batchnorm Loss(Acc): 0.02019(0.99)\n",
      "Batchnorm Loss(Acc): 0.02019(0.99), TP: 7017, TN: 11158, FP, 25, FN: 152\n",
      "\n",
      "[Epoch 1-TRAIN] Batchnorm Loss(Acc): 0.01516(0.99)\n",
      "Batchnorm Loss(Acc): 0.01516(0.99), TP: 133088, TN: 213384, FP, 325, FN: 1879\n",
      "[Epoch 1-VALID] Batchnorm Loss(Acc): 0.01818(0.99)\n",
      "Batchnorm Loss(Acc): 0.01818(0.99), TP: 7059, TN: 11160, FP, 23, FN: 110\n",
      "\n",
      "[Epoch 2-TRAIN] Batchnorm Loss(Acc): 0.01815(0.99)\n",
      "Batchnorm Loss(Acc): 0.01815(0.99), TP: 132307, TN: 213423, FP, 286, FN: 2660\n",
      "[Epoch 2-VALID] Batchnorm Loss(Acc): 0.02204(0.99)\n",
      "Batchnorm Loss(Acc): 0.02204(0.99), TP: 7011, TN: 11164, FP, 19, FN: 158\n",
      "\n",
      "[Epoch 3-TRAIN] Batchnorm Loss(Acc): 0.01197(1.00)\n",
      "Batchnorm Loss(Acc): 0.01197(1.00), TP: 134360, TN: 213148, FP, 561, FN: 607\n",
      "[Epoch 3-VALID] Batchnorm Loss(Acc): 0.01754(0.99)\n",
      "Batchnorm Loss(Acc): 0.01754(0.99), TP: 7120, TN: 11138, FP, 45, FN: 49\n",
      "\n",
      "[Epoch 4-TRAIN] Batchnorm Loss(Acc): 0.01081(1.00)\n",
      "Batchnorm Loss(Acc): 0.01081(1.00), TP: 133868, TN: 213372, FP, 337, FN: 1099\n",
      "[Epoch 4-VALID] Batchnorm Loss(Acc): 0.01681(0.99)\n",
      "Batchnorm Loss(Acc): 0.01681(0.99), TP: 7094, TN: 11157, FP, 26, FN: 75\n",
      "\n",
      "[Epoch 5-TRAIN] Batchnorm Loss(Acc): 0.00834(1.00)\n",
      "Batchnorm Loss(Acc): 0.00834(1.00), TP: 134253, TN: 213357, FP, 352, FN: 714\n",
      "[Epoch 5-VALID] Batchnorm Loss(Acc): 0.01541(0.99)\n",
      "Batchnorm Loss(Acc): 0.01541(0.99), TP: 7108, TN: 11150, FP, 33, FN: 61\n",
      "\n",
      "[Epoch 6-TRAIN] Batchnorm Loss(Acc): 0.00745(1.00)\n",
      "Batchnorm Loss(Acc): 0.00745(1.00), TP: 134352, TN: 213391, FP, 318, FN: 615\n",
      "[Epoch 6-VALID] Batchnorm Loss(Acc): 0.01476(1.00)\n",
      "Batchnorm Loss(Acc): 0.01476(1.00), TP: 7112, TN: 11157, FP, 26, FN: 57\n",
      "\n",
      "[Epoch 7-TRAIN] Batchnorm Loss(Acc): 0.00669(1.00)\n",
      "Batchnorm Loss(Acc): 0.00669(1.00), TP: 134321, TN: 213431, FP, 278, FN: 646\n",
      "[Epoch 7-VALID] Batchnorm Loss(Acc): 0.01486(1.00)\n",
      "Batchnorm Loss(Acc): 0.01486(1.00), TP: 7111, TN: 11159, FP, 24, FN: 58\n",
      "\n",
      "[Epoch 8-TRAIN] Batchnorm Loss(Acc): 0.00643(1.00)\n",
      "Batchnorm Loss(Acc): 0.00643(1.00), TP: 134450, TN: 213430, FP, 279, FN: 517\n",
      "[Epoch 8-VALID] Batchnorm Loss(Acc): 0.01550(1.00)\n",
      "Batchnorm Loss(Acc): 0.01550(1.00), TP: 7119, TN: 11159, FP, 24, FN: 50\n",
      "\n",
      "[Epoch 9-TRAIN] Batchnorm Loss(Acc): 0.00687(1.00)\n",
      "Batchnorm Loss(Acc): 0.00687(1.00), TP: 134280, TN: 213448, FP, 261, FN: 687\n",
      "[Epoch 9-VALID] Batchnorm Loss(Acc): 0.01604(1.00)\n",
      "Batchnorm Loss(Acc): 0.01604(1.00), TP: 7108, TN: 11164, FP, 19, FN: 61\n",
      "\n",
      "[Epoch 10-TRAIN] Batchnorm Loss(Acc): 0.00639(1.00)\n",
      "Batchnorm Loss(Acc): 0.00639(1.00), TP: 134386, TN: 213429, FP, 280, FN: 581\n",
      "[Epoch 10-VALID] Batchnorm Loss(Acc): 0.01584(1.00)\n",
      "Batchnorm Loss(Acc): 0.01584(1.00), TP: 7116, TN: 11161, FP, 22, FN: 53\n",
      "\n",
      "[Epoch 11-TRAIN] Batchnorm Loss(Acc): 0.00708(1.00)\n",
      "Batchnorm Loss(Acc): 0.00708(1.00), TP: 134135, TN: 213460, FP, 249, FN: 832\n",
      "[Epoch 11-VALID] Batchnorm Loss(Acc): 0.01739(1.00)\n",
      "Batchnorm Loss(Acc): 0.01739(1.00), TP: 7100, TN: 11164, FP, 19, FN: 69\n",
      "\n",
      "[Epoch 12-TRAIN] Batchnorm Loss(Acc): 0.00588(1.00)\n",
      "Batchnorm Loss(Acc): 0.00588(1.00), TP: 134539, TN: 213434, FP, 275, FN: 428\n",
      "[Epoch 12-VALID] Batchnorm Loss(Acc): 0.01637(1.00)\n",
      "Batchnorm Loss(Acc): 0.01637(1.00), TP: 7123, TN: 11155, FP, 28, FN: 46\n",
      "\n",
      "[Epoch 13-TRAIN] Batchnorm Loss(Acc): 0.00573(1.00)\n",
      "Batchnorm Loss(Acc): 0.00573(1.00), TP: 134539, TN: 213416, FP, 293, FN: 428\n",
      "[Epoch 13-VALID] Batchnorm Loss(Acc): 0.01644(1.00)\n",
      "Batchnorm Loss(Acc): 0.01644(1.00), TP: 7119, TN: 11153, FP, 30, FN: 50\n",
      "\n",
      "[Epoch 14-TRAIN] Batchnorm Loss(Acc): 0.00587(1.00)\n",
      "Batchnorm Loss(Acc): 0.00587(1.00), TP: 134480, TN: 213455, FP, 254, FN: 487\n",
      "[Epoch 14-VALID] Batchnorm Loss(Acc): 0.01675(1.00)\n",
      "Batchnorm Loss(Acc): 0.01675(1.00), TP: 7119, TN: 11161, FP, 22, FN: 50\n",
      "\n",
      "[Epoch 15-TRAIN] Batchnorm Loss(Acc): 0.00583(1.00)\n",
      "Batchnorm Loss(Acc): 0.00583(1.00), TP: 134423, TN: 213488, FP, 221, FN: 544\n",
      "[Epoch 15-VALID] Batchnorm Loss(Acc): 0.01684(1.00)\n",
      "Batchnorm Loss(Acc): 0.01684(1.00), TP: 7114, TN: 11159, FP, 24, FN: 55\n",
      "\n",
      "[Epoch 16-TRAIN] Batchnorm Loss(Acc): 0.00582(1.00)\n",
      "Batchnorm Loss(Acc): 0.00582(1.00), TP: 134449, TN: 213495, FP, 214, FN: 518\n",
      "[Epoch 16-VALID] Batchnorm Loss(Acc): 0.01737(1.00)\n",
      "Batchnorm Loss(Acc): 0.01737(1.00), TP: 7115, TN: 11159, FP, 24, FN: 54\n",
      "\n",
      "[Epoch 17-TRAIN] Batchnorm Loss(Acc): 0.00514(1.00)\n",
      "Batchnorm Loss(Acc): 0.00514(1.00), TP: 134499, TN: 213472, FP, 237, FN: 468\n",
      "[Epoch 17-VALID] Batchnorm Loss(Acc): 0.01751(1.00)\n",
      "Batchnorm Loss(Acc): 0.01751(1.00), TP: 7116, TN: 11161, FP, 22, FN: 53\n",
      "\n",
      "[Epoch 18-TRAIN] Batchnorm Loss(Acc): 0.00511(1.00)\n",
      "Batchnorm Loss(Acc): 0.00511(1.00), TP: 134609, TN: 213460, FP, 249, FN: 358\n",
      "[Epoch 18-VALID] Batchnorm Loss(Acc): 0.01774(1.00)\n",
      "Batchnorm Loss(Acc): 0.01774(1.00), TP: 7123, TN: 11160, FP, 23, FN: 46\n",
      "\n",
      "[Epoch 19-TRAIN] Batchnorm Loss(Acc): 0.00553(1.00)\n",
      "Batchnorm Loss(Acc): 0.00553(1.00), TP: 134704, TN: 213328, FP, 381, FN: 263\n",
      "[Epoch 19-VALID] Batchnorm Loss(Acc): 0.01722(1.00)\n",
      "Batchnorm Loss(Acc): 0.01722(1.00), TP: 7134, TN: 11148, FP, 35, FN: 35\n",
      "\n",
      "[Epoch 20-TRAIN] Batchnorm Loss(Acc): 0.00527(1.00)\n",
      "Batchnorm Loss(Acc): 0.00527(1.00), TP: 134497, TN: 213469, FP, 240, FN: 470\n",
      "[Epoch 20-VALID] Batchnorm Loss(Acc): 0.01844(1.00)\n",
      "Batchnorm Loss(Acc): 0.01844(1.00), TP: 7114, TN: 11160, FP, 23, FN: 55\n",
      "\n",
      "[Epoch 21-TRAIN] Batchnorm Loss(Acc): 0.00480(1.00)\n",
      "Batchnorm Loss(Acc): 0.00480(1.00), TP: 134675, TN: 213423, FP, 286, FN: 292\n",
      "[Epoch 21-VALID] Batchnorm Loss(Acc): 0.01749(1.00)\n",
      "Batchnorm Loss(Acc): 0.01749(1.00), TP: 7125, TN: 11158, FP, 25, FN: 44\n",
      "\n",
      "[Epoch 22-TRAIN] Batchnorm Loss(Acc): 0.00518(1.00)\n",
      "Batchnorm Loss(Acc): 0.00518(1.00), TP: 134651, TN: 213423, FP, 286, FN: 316\n",
      "[Epoch 22-VALID] Batchnorm Loss(Acc): 0.01730(1.00)\n",
      "Batchnorm Loss(Acc): 0.01730(1.00), TP: 7121, TN: 11158, FP, 25, FN: 48\n",
      "\n",
      "[Epoch 23-TRAIN] Batchnorm Loss(Acc): 0.00483(1.00)\n",
      "Batchnorm Loss(Acc): 0.00483(1.00), TP: 134687, TN: 213435, FP, 274, FN: 280\n",
      "[Epoch 23-VALID] Batchnorm Loss(Acc): 0.01834(1.00)\n",
      "Batchnorm Loss(Acc): 0.01834(1.00), TP: 7123, TN: 11158, FP, 25, FN: 46\n",
      "\n",
      "[Epoch 24-TRAIN] Batchnorm Loss(Acc): 0.00532(1.00)\n",
      "Batchnorm Loss(Acc): 0.00532(1.00), TP: 134642, TN: 213379, FP, 330, FN: 325\n",
      "[Epoch 24-VALID] Batchnorm Loss(Acc): 0.01836(1.00)\n",
      "Batchnorm Loss(Acc): 0.01836(1.00), TP: 7124, TN: 11150, FP, 33, FN: 45\n",
      "\n",
      "[Epoch 25-TRAIN] Batchnorm Loss(Acc): 0.00508(1.00)\n",
      "Batchnorm Loss(Acc): 0.00508(1.00), TP: 134586, TN: 213476, FP, 233, FN: 381\n",
      "[Epoch 25-VALID] Batchnorm Loss(Acc): 0.01771(1.00)\n",
      "Batchnorm Loss(Acc): 0.01771(1.00), TP: 7120, TN: 11153, FP, 30, FN: 49\n",
      "\n",
      "[Epoch 26-TRAIN] Batchnorm Loss(Acc): 0.00548(1.00)\n",
      "Batchnorm Loss(Acc): 0.00548(1.00), TP: 134402, TN: 213491, FP, 218, FN: 565\n",
      "[Epoch 26-VALID] Batchnorm Loss(Acc): 0.02005(1.00)\n",
      "Batchnorm Loss(Acc): 0.02005(1.00), TP: 7105, TN: 11157, FP, 26, FN: 64\n",
      "\n",
      "[Epoch 27-TRAIN] Batchnorm Loss(Acc): 0.00537(1.00)\n",
      "Batchnorm Loss(Acc): 0.00537(1.00), TP: 134488, TN: 213464, FP, 245, FN: 479\n",
      "[Epoch 27-VALID] Batchnorm Loss(Acc): 0.01803(1.00)\n",
      "Batchnorm Loss(Acc): 0.01803(1.00), TP: 7116, TN: 11160, FP, 23, FN: 53\n",
      "\n",
      "[Epoch 28-TRAIN] Batchnorm Loss(Acc): 0.00511(1.00)\n",
      "Batchnorm Loss(Acc): 0.00511(1.00), TP: 134608, TN: 213449, FP, 260, FN: 359\n",
      "[Epoch 28-VALID] Batchnorm Loss(Acc): 0.01821(1.00)\n",
      "Batchnorm Loss(Acc): 0.01821(1.00), TP: 7118, TN: 11156, FP, 27, FN: 51\n",
      "\n",
      "[Epoch 29-TRAIN] Batchnorm Loss(Acc): 0.00578(1.00)\n",
      "Batchnorm Loss(Acc): 0.00578(1.00), TP: 134744, TN: 213273, FP, 436, FN: 223\n",
      "[Epoch 29-VALID] Batchnorm Loss(Acc): 0.01931(1.00)\n",
      "Batchnorm Loss(Acc): 0.01931(1.00), TP: 7132, TN: 11142, FP, 41, FN: 37\n",
      "\n",
      "[Epoch 30-TRAIN] Batchnorm Loss(Acc): 0.00656(1.00)\n",
      "Batchnorm Loss(Acc): 0.00656(1.00), TP: 134751, TN: 213179, FP, 530, FN: 216\n",
      "[Epoch 30-VALID] Batchnorm Loss(Acc): 0.01998(1.00)\n",
      "Batchnorm Loss(Acc): 0.01998(1.00), TP: 7132, TN: 11129, FP, 54, FN: 37\n",
      "\n",
      "[Epoch 31-TRAIN] Batchnorm Loss(Acc): 0.00554(1.00)\n",
      "Batchnorm Loss(Acc): 0.00554(1.00), TP: 134556, TN: 213446, FP, 263, FN: 411\n",
      "[Epoch 31-VALID] Batchnorm Loss(Acc): 0.01806(1.00)\n",
      "Batchnorm Loss(Acc): 0.01806(1.00), TP: 7117, TN: 11158, FP, 25, FN: 52\n",
      "\n",
      "[Epoch 32-TRAIN] Batchnorm Loss(Acc): 0.00577(1.00)\n",
      "Batchnorm Loss(Acc): 0.00577(1.00), TP: 134559, TN: 213422, FP, 287, FN: 408\n",
      "[Epoch 32-VALID] Batchnorm Loss(Acc): 0.01927(1.00)\n",
      "Batchnorm Loss(Acc): 0.01927(1.00), TP: 7118, TN: 11150, FP, 33, FN: 51\n",
      "\n",
      "[Epoch 33-TRAIN] Batchnorm Loss(Acc): 0.00590(1.00)\n",
      "Batchnorm Loss(Acc): 0.00590(1.00), TP: 134606, TN: 213425, FP, 284, FN: 361\n",
      "[Epoch 33-VALID] Batchnorm Loss(Acc): 0.01928(1.00)\n",
      "Batchnorm Loss(Acc): 0.01928(1.00), TP: 7120, TN: 11147, FP, 36, FN: 49\n",
      "\n",
      "[Epoch 34-TRAIN] Batchnorm Loss(Acc): 0.00539(1.00)\n",
      "Batchnorm Loss(Acc): 0.00539(1.00), TP: 134725, TN: 213324, FP, 385, FN: 242\n",
      "[Epoch 34-VALID] Batchnorm Loss(Acc): 0.01902(1.00)\n",
      "Batchnorm Loss(Acc): 0.01902(1.00), TP: 7128, TN: 11142, FP, 41, FN: 41\n",
      "\n",
      "[Epoch 35-TRAIN] Batchnorm Loss(Acc): 0.00580(1.00)\n",
      "Batchnorm Loss(Acc): 0.00580(1.00), TP: 134642, TN: 213361, FP, 348, FN: 325\n",
      "[Epoch 35-VALID] Batchnorm Loss(Acc): 0.01954(1.00)\n",
      "Batchnorm Loss(Acc): 0.01954(1.00), TP: 7121, TN: 11148, FP, 35, FN: 48\n",
      "\n",
      "[Epoch 36-TRAIN] Batchnorm Loss(Acc): 0.00560(1.00)\n",
      "Batchnorm Loss(Acc): 0.00560(1.00), TP: 134540, TN: 213471, FP, 238, FN: 427\n",
      "[Epoch 36-VALID] Batchnorm Loss(Acc): 0.02063(1.00)\n",
      "Batchnorm Loss(Acc): 0.02063(1.00), TP: 7115, TN: 11159, FP, 24, FN: 54\n",
      "\n",
      "[Epoch 37-TRAIN] Batchnorm Loss(Acc): 0.00621(1.00)\n",
      "Batchnorm Loss(Acc): 0.00621(1.00), TP: 134633, TN: 213338, FP, 371, FN: 334\n",
      "[Epoch 37-VALID] Batchnorm Loss(Acc): 0.01996(1.00)\n",
      "Batchnorm Loss(Acc): 0.01996(1.00), TP: 7126, TN: 11138, FP, 45, FN: 43\n",
      "\n",
      "[Epoch 38-TRAIN] Batchnorm Loss(Acc): 0.00516(1.00)\n",
      "Batchnorm Loss(Acc): 0.00516(1.00), TP: 134741, TN: 213342, FP, 367, FN: 226\n",
      "[Epoch 38-VALID] Batchnorm Loss(Acc): 0.01993(1.00)\n",
      "Batchnorm Loss(Acc): 0.01993(1.00), TP: 7130, TN: 11147, FP, 36, FN: 39\n",
      "\n",
      "[Epoch 39-TRAIN] Batchnorm Loss(Acc): 0.00588(1.00)\n",
      "Batchnorm Loss(Acc): 0.00588(1.00), TP: 134600, TN: 213391, FP, 318, FN: 367\n",
      "[Epoch 39-VALID] Batchnorm Loss(Acc): 0.02031(1.00)\n",
      "Batchnorm Loss(Acc): 0.02031(1.00), TP: 7121, TN: 11149, FP, 34, FN: 48\n",
      "\n",
      "[Epoch 40-TRAIN] Batchnorm Loss(Acc): 0.00659(1.00)\n",
      "Batchnorm Loss(Acc): 0.00659(1.00), TP: 134746, TN: 213253, FP, 456, FN: 221\n",
      "[Epoch 40-VALID] Batchnorm Loss(Acc): 0.02133(1.00)\n",
      "Batchnorm Loss(Acc): 0.02133(1.00), TP: 7139, TN: 11128, FP, 55, FN: 30\n",
      "\n",
      "[Epoch 41-TRAIN] Batchnorm Loss(Acc): 0.00508(1.00)\n",
      "Batchnorm Loss(Acc): 0.00508(1.00), TP: 134555, TN: 213462, FP, 247, FN: 412\n",
      "[Epoch 41-VALID] Batchnorm Loss(Acc): 0.01899(1.00)\n",
      "Batchnorm Loss(Acc): 0.01899(1.00), TP: 7118, TN: 11154, FP, 29, FN: 51\n",
      "\n",
      "[Epoch 42-TRAIN] Batchnorm Loss(Acc): 0.00588(1.00)\n",
      "Batchnorm Loss(Acc): 0.00588(1.00), TP: 134542, TN: 213441, FP, 268, FN: 425\n",
      "[Epoch 42-VALID] Batchnorm Loss(Acc): 0.02167(1.00)\n",
      "Batchnorm Loss(Acc): 0.02167(1.00), TP: 7123, TN: 11151, FP, 32, FN: 46\n",
      "\n",
      "[Epoch 43-TRAIN] Batchnorm Loss(Acc): 0.00496(1.00)\n",
      "Batchnorm Loss(Acc): 0.00496(1.00), TP: 134569, TN: 213509, FP, 200, FN: 398\n",
      "[Epoch 43-VALID] Batchnorm Loss(Acc): 0.01882(1.00)\n",
      "Batchnorm Loss(Acc): 0.01882(1.00), TP: 7120, TN: 11156, FP, 27, FN: 49\n",
      "\n",
      "[Epoch 44-TRAIN] Batchnorm Loss(Acc): 0.00639(1.00)\n",
      "Batchnorm Loss(Acc): 0.00639(1.00), TP: 134679, TN: 213308, FP, 401, FN: 288\n",
      "[Epoch 44-VALID] Batchnorm Loss(Acc): 0.01947(1.00)\n",
      "Batchnorm Loss(Acc): 0.01947(1.00), TP: 7133, TN: 11144, FP, 39, FN: 36\n",
      "\n",
      "[Epoch 45-TRAIN] Batchnorm Loss(Acc): 0.00598(1.00)\n",
      "Batchnorm Loss(Acc): 0.00598(1.00), TP: 134605, TN: 213338, FP, 371, FN: 362\n",
      "[Epoch 45-VALID] Batchnorm Loss(Acc): 0.02072(1.00)\n",
      "Batchnorm Loss(Acc): 0.02072(1.00), TP: 7120, TN: 11147, FP, 36, FN: 49\n",
      "\n",
      "[Epoch 46-TRAIN] Batchnorm Loss(Acc): 0.00599(1.00)\n",
      "Batchnorm Loss(Acc): 0.00599(1.00), TP: 134670, TN: 213372, FP, 337, FN: 297\n",
      "[Epoch 46-VALID] Batchnorm Loss(Acc): 0.01939(1.00)\n",
      "Batchnorm Loss(Acc): 0.01939(1.00), TP: 7128, TN: 11149, FP, 34, FN: 41\n",
      "\n",
      "[Epoch 47-TRAIN] Batchnorm Loss(Acc): 0.00629(1.00)\n",
      "Batchnorm Loss(Acc): 0.00629(1.00), TP: 134661, TN: 213340, FP, 369, FN: 306\n",
      "[Epoch 47-VALID] Batchnorm Loss(Acc): 0.01995(1.00)\n",
      "Batchnorm Loss(Acc): 0.01995(1.00), TP: 7130, TN: 11140, FP, 43, FN: 39\n",
      "\n",
      "[Epoch 48-TRAIN] Batchnorm Loss(Acc): 0.00546(1.00)\n",
      "Batchnorm Loss(Acc): 0.00546(1.00), TP: 134665, TN: 213417, FP, 292, FN: 302\n",
      "[Epoch 48-VALID] Batchnorm Loss(Acc): 0.02008(1.00)\n",
      "Batchnorm Loss(Acc): 0.02008(1.00), TP: 7126, TN: 11154, FP, 29, FN: 43\n",
      "\n",
      "[Epoch 49-TRAIN] Batchnorm Loss(Acc): 0.00536(1.00)\n",
      "Batchnorm Loss(Acc): 0.00536(1.00), TP: 134723, TN: 213426, FP, 283, FN: 244\n",
      "[Epoch 49-VALID] Batchnorm Loss(Acc): 0.01983(1.00)\n",
      "Batchnorm Loss(Acc): 0.01983(1.00), TP: 7135, TN: 11149, FP, 34, FN: 34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    # Train\n",
    "    for inds in minibatches[:-1]:\n",
    "        X_batch = X_train[inds, :]\n",
    "        y_batch = y_train[inds]\n",
    "        _, bn_loss = bn_solver.train(X_batch, y_batch)\n",
    "    \n",
    "    bn_solver.epoch = epoch\n",
    "    bn_solver.save(epoch)\n",
    "    \n",
    "    # Train Loss\n",
    "    b_loss, b_acc, b_tp, b_tn, b_fp, b_fn = bn_solver.evaluate(X_train, y_train, batch_size)\n",
    "    train_losses.append(b_loss)\n",
    "    train_accs.append(b_acc)\n",
    "    train_ROC.append([b_tp, b_tn, b_fp, b_fn])\n",
    "    print('[Epoch %d-TRAIN] Batchnorm Loss(Acc): %.5f(%.2f)' % (epoch, b_loss, b_acc))\n",
    "    print('Batchnorm Loss(Acc): %.5f(%.2f), TP: %d, TN: %d, FP, %d, FN: %d' % (b_loss, b_acc,b_tp, b_tn, b_fp, b_fn))\n",
    "\n",
    "    # Evaluate \n",
    "    b_loss, b_acc, b_tp, b_tn, b_fp, b_fn = bn_solver.evaluate(X_valid, y_valid)\n",
    "    valid_losses.append(b_loss)\n",
    "    valid_accs.append(b_acc)\n",
    "    valid_ROC.append([b_tp, b_tn, b_fp, b_fn])\n",
    "    print('[Epoch %d-VALID] Batchnorm Loss(Acc): %.5f(%.2f)' % (epoch, b_loss, b_acc))\n",
    "    print('Batchnorm Loss(Acc): %.5f(%.2f), TP: %d, TN: %d, FP, %d, FN: %d' % (b_loss, b_acc,b_tp, b_tn, b_fp, b_fn))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create Embedding on tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Embedding ~\n",
    "bn_solver.compute_embedding(X_valid, y_valid, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206762, 772)\n",
      "(206762, 1)\n"
     ]
    }
   ],
   "source": [
    "# load test\n",
    "test_malicious = np.genfromtxt(\"./JSF20170324/test/malicious.vlog\", delimiter=',')\n",
    "test_normal    = np.genfromtxt(\"./JSF20170324/test/normal.vlog\"   , delimiter=',')\n",
    "\n",
    "X_test = np.concatenate((test_malicious, test_normal))\n",
    "y_test = np.array([1]*len(test_malicious) + [0]*len(test_normal))\n",
    "del(test_malicious)\n",
    "del(test_normal)\n",
    "y_test = np.reshape(y_test, [len(y_test), 1])\n",
    "X_test = np.delete(X_test, np.s_[:1], axis=1)\n",
    "X_test = np.log(X_test+1)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b_loss, b_acc, b_tp, b_tn, b_fp, b_fn = bn_solver.evaluate(X_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.144941735078 0.963595872329 86896.0 112339.0 827.0 6700.0\n",
      "Test Result : \n",
      "- accuract        :0.963596\n",
      "- True positive   :86896\n",
      "- True negative   :112339\n",
      "- False positive  :827\n",
      "- False negative  :6700\n",
      "- Precision       :0.990573\n",
      "- Recall          :0.928416\n"
     ]
    }
   ],
   "source": [
    "print(b_loss, b_acc, b_tp, b_tn, b_fp, b_fn)\n",
    "print(\"Test Result : \")\n",
    "print(\"- accuract        :%f\" % b_acc)\n",
    "print(\"- True positive   :%d\" % b_tp)\n",
    "print(\"- True negative   :%d\" % b_tn)\n",
    "print(\"- False positive  :%d\" % b_fp)\n",
    "print(\"- False negative  :%d\" % b_fn)\n",
    "print(\"- Precision       :%f\" % (b_tp/(b_tp+b_fp)))\n",
    "print(\"- Recall          :%f\" % (b_tp/(b_tp+b_fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accrossing to above, maybe out model not generalize enought"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
